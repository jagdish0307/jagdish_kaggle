{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')\n",
    "sample_df=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_df\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "train_df['keyword'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['location'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test_df\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['keyword'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['location'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test_df\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pie(train_df['target'].value_counts(), labels=['Non-disaster', 'Disaster'], autopct='%0.2f')\n",
    "plt.legend()  # Adds a legend\n",
    "plt.title('Distribution of Disaster Tweets')  # Adds a descriptive title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random state\n",
    "random_state = 4041\n",
    "\n",
    "# import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# concat all the text for each labels\n",
    "non_disaster_text = [''.join(t) for t in train_df[train_df['target']==0]['text']]\n",
    "non_disaster_strings = ' '.join(map(str, non_disaster_text))\n",
    "disaster_text = [''.join(t) for t in train_df[train_df['target']==1]['text']]\n",
    "disaster_strings = ' '.join(map(str, disaster_text))\n",
    "\n",
    "# generate word clouds\n",
    "non_disaster_cloud = WordCloud(width=800, height=400, max_words=500, background_color='white', random_state=random_state).generate(non_disaster_strings)\n",
    "disaster_cloud = WordCloud(width=800, height=400, max_words=500, random_state=random_state).generate(disaster_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots for the generated clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize = (20,20))\n",
    "axes[0].imshow(non_disaster_cloud, interpolation='bilinear')\n",
    "axes[1].imshow(disaster_cloud, interpolation='bilinear')\n",
    "\n",
    "# turn the axis off\n",
    "[ax.axis('off') for ax in axes]\n",
    "\n",
    "# add titles\n",
    "axes[0].set_title('Non-disaster Tweets', fontsize=16)\n",
    "axes[1].set_title('Disaster Tweets', fontsize=16)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-remove urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    text = re.sub(r'((?:https?|ftp|file)://[-\\w\\d+=&@#/%?~|!:;\\.,]*)', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text'].apply(remove_url)\n",
    "test_df['text_cleaned'] = test_df['text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_HTML)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rmove Characters References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(text):\n",
    "    text = re.sub(r'&[a-zA-Z]+;?', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_references)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Non-printable Characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.printable\n",
    "\n",
    "def remove_non_printable(text):\n",
    "    text = ''.join([word for word in text if word in string.printable])\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_non_printable)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_non_printable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Numeric Values\n",
    "Remove numeric values, including mixtures of alphabetical characters and numeric values such as 'M194', '5km'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_num)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_num)\n",
    "\n",
    "train_df.tail()\n",
    "\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Below are 10 features we're going to create:\n",
    "\n",
    "- Number of sentences\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Number of hashtags\n",
    "- Number of mentions\n",
    "- Number of all caps words\n",
    "- Average length of words\n",
    "- Number of proper nouns (PROPN)\n",
    "- Number of non-proper nouns (NOUN)\n",
    "- Percentage of characters that are punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Number of SentencesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# create a new feature for the number of sentences in each Tweet\n",
    "train_df['sent_count'] = train_df['text'].apply(nltk.tokenize.sent_tokenize).apply(len)\n",
    "test_df['sent_count'] = test_df['text'].apply(nltk.tokenize.sent_tokenize).apply(len)\n",
    "\n",
    "# create a new feature for the number of words\n",
    "train_df['word_count'] = train_df['text'].apply(nltk.tokenize.word_tokenize).apply(len)\n",
    "test_df['word_count'] = test_df['text'].apply(nltk.tokenize.word_tokenize).apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature for the number of characters excluding white spaces\n",
    "train_df['char_count'] = train_df['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "test_df['char_count'] = test_df['text'].apply(lambda x: len(x) - x.count(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns the number of hashtags in a string\n",
    "def hash_count(string):\n",
    "    words = string.split()\n",
    "    hashtags = [w for w in words if w.startswith('#')]\n",
    "    return len(hashtags)\n",
    "\n",
    "# create a new feature for the number of hashtags\n",
    "train_df['hash_count'] = train_df['text'].apply(hash_count)\n",
    "test_df['hash_count'] = test_df['text'].apply(hash_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the number of mentions in a string\n",
    "def ment_count(string):\n",
    "    words = string.split()\n",
    "    mentions = [w for w in words if w.startswith('@')]\n",
    "    return len(mentions)\n",
    "\n",
    "# create a new feature for the number of mentions\n",
    "train_df['ment_count'] = train_df['text'].apply(ment_count)\n",
    "test_df['ment_count'] = test_df['text'].apply(ment_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of All Caps Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def all_caps_count(string):\n",
    "    words = string.split()\n",
    "    pattern = re.compile(r'\\b[A-Z]{2,}\\b')  # Matches words with 2 or more consecutive uppercase letters\n",
    "    caps_words = [word for word in words if pattern.fullmatch(word)]\n",
    "    return len(caps_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Length of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the average length of words\n",
    "def avg_word_len(string):\n",
    "    words = string.split()\n",
    "    total_len = sum([len(words[i]) for i in range(len(words))])\n",
    "    avg_len = round(total_len / len(words), 2)\n",
    "    return avg_len\n",
    "\n",
    "# create a new feature for the average length of words\n",
    "train_df['avg_word_len'] = train_df['text'].apply(avg_word_len)\n",
    "test_df['avg_word_len'] = test_df['text'].apply(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# define a function using nltk that returns the number of proper nouns in the text\n",
    "def propn_count_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = [token for token in nltk.pos_tag(tokens)]\n",
    "    propn_count = len([token for (token, tag) in tagged if tag == 'NNP' or tag == 'NNPS'])\n",
    "    return propn_count\n",
    "\n",
    "# create a new feature for the number of proper nouns\n",
    "train_df['propn_count_nltk'] = train_df['text'].apply(propn_count_nltk)\n",
    "test_df['propn_count_nltk'] = test_df['text'].apply(propn_count_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned', 'propn_count_nltk']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test how nltk worked with the first text\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token, tag) for (token, tag) in nltk.pos_tag(nltk.word_tokenize(string)) if tag == 'NNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test how nltk works with the first text after lowercasing it\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token, tag) for (token, tag) in nltk.pos_tag(nltk.word_tokenize(string.lower())) if tag == 'NNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# check the same string with spaCy\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token.text, token.pos_) for token in nlp(string) if token.pos_=='PROPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns number of proper nouns with spaCy\n",
    "def propn_count(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "# create a new feature for numbers of proper nouns\n",
    "train_df['propn_count'] = train_df['text'].apply(propn_count)\n",
    "test_df['propn_count'] = test_df['text'].apply(propn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove 'propn_count_nltk' columns\n",
    "train_df = train_df.drop(['propn_count_nltk'], axis=1)\n",
    "test_df = test_df.drop(['propn_count_nltk'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned', 'propn_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Non-proper Nouns (NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns number of non-proper nouns\n",
    "def noun_count(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "# create a new feature for numbers of non-proper nouns\n",
    "train_df['noun_count'] = train_df['text'].apply(noun_count)\n",
    "test_df['noun_count'] = test_df['text'].apply(noun_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of Characters that are Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# define a function that returns the percentage of punctuation\n",
    "def punc_per(text):\n",
    "    total_count = len(text) - text.count(\" \")\n",
    "    punc_count = sum([1 for c in text if c in string.punctuation])\n",
    "    if punc_count != 0 and total_count != 0:\n",
    "        return round(punc_count / total_count * 100, 2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# create a new feature for the percentage of punctuation in text\n",
    "train_df['punc_per'] = train_df['text'].apply(punc_per)\n",
    "test_df['punc_per'] = test_df['text'].apply(punc_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "test_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Part II\n",
    "\n",
    "Let's resume our text preprocessing and lemmatize the text and make it lowercase.\n",
    "\n",
    "We'll also remove repeated characters in elongated words, as well as mentions, stopwords, and punctuation.\n",
    "We'll keep hashtags as they may provide valuable insights in this particular project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x:' '.join([t.lemma_ for t in nlp(x)]))\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(lambda x:' '.join([t.lemma_ for t in nlp(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase the text\n",
    "train_df['text_cleaned'] = [t.lower() for t in train_df['text_cleaned']]\n",
    "test_df['text_cleaned'] = [t.lower() for t in test_df['text_cleaned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Repeated Charcters in Elongated Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that removes repeated characters in elongated words\n",
    "def remove_repeated(text):\n",
    "    elongated = re.compile(r'(\\S*?)([a-z])\\2{2,}')\n",
    "    text = elongated.sub(r'\\1\\2', text)\n",
    "    return text\n",
    "\n",
    "# remove repeated characters in elongated words\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_repeated)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_repeated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Mentions\n",
    "List item\n",
    "\n",
    "List item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that removes mentions\n",
    "def remove_mention(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove mentions\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_mention)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that removes stopwords\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    text_nostop = ' '.join([token for token in text.split() if token not in stopwords])\n",
    "    return text_nostop\n",
    "\n",
    "# remove stopwords\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_stopwords)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation\n",
    " List item\n",
    " \n",
    " List item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    punct = string.punctuation\n",
    "    text_nospunct = ' '.join([token for token in text.split() if token not in punct])\n",
    "    return text_nospunct\n",
    "\n",
    "# remove punctuation\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_punct)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw vs. Preprocessed Text with Word Clouds\n",
    "Let's generate word clouds of preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the preprocessed text for both labels\n",
    "non_disaster_processed = [''.join(t) for t in train_df[train_df['target']==0]['text_cleaned']]\n",
    "non_disaster_processed_s = ' '.join(map(str, non_disaster_processed))\n",
    "disaster_processed = [''.join(t) for t in train_df[train_df['target']==1]['text_cleaned']]\n",
    "disaster_processed_s = ' '.join(map(str, disaster_processed))\n",
    "\n",
    "# generate word clouds of the preprocessed text\n",
    "non_disaster_processed_wc = WordCloud(width=800, height=400, max_words=500, background_color='white', random_state=random_state).generate(non_disaster_processed_s)\n",
    "disaster_processed_wc = WordCloud(width=800, height=400, max_words=500, random_state=random_state).generate(disaster_processed_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots for the generated clouds\n",
    "fig, axes = plt.subplots(2, 2, figsize = (20,10))\n",
    "axes[0,0].imshow(non_disaster_cloud, interpolation='bilinear')\n",
    "axes[0,1].imshow(disaster_cloud, interpolation='bilinear')\n",
    "axes[1,0].imshow(non_disaster_processed_wc, interpolation='bilinear')\n",
    "axes[1,1].imshow(disaster_processed_wc, interpolation='bilinear')\n",
    "\n",
    "# turn the axis off\n",
    "[ax.axis('off') for ax in axes.ravel()]\n",
    "\n",
    "# add titles\n",
    "axes[0,0].set_title('Non-disaster Tweets (raw)', fontsize=16)\n",
    "axes[0,1].set_title('Disaster Tweets (raw)', fontsize=16)\n",
    "axes[1,0].set_title('Non-disaster Tweets (preprocessed)', fontsize=16)\n",
    "axes[1,1].set_title('Disaster Tweets (preprocessed)', fontsize=16)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's easier to see the frequently used words that actually are meaningful. It also seems like more disaster-related words are showing on the word cloud of real disaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Visualizing Differences**\n",
    "Let's visualize some of the features we've created and see if there are easy-to-tell differences between disaster and non-disaster Tweets in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features exist in the DataFrame\n",
    "features = ['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count', 'all_caps_count',\n",
    "            'avg_word_len', 'propn_count', 'noun_count', 'punc_per']\n",
    "\n",
    "# Remove features not in the DataFrame\n",
    "features = [feature for feature in features if feature in train_df.columns]\n",
    "\n",
    "# Create the figure\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Adjust the height of the padding between subplots to avoid overlapping\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Add a centered suptitle to the figure\n",
    "plt.suptitle(\"Difference in Features, Disaster vs. Non-disaster\", fontsize=20, y=0.91)\n",
    "\n",
    "# Generate the histograms in a for loop\n",
    "for i, feature in enumerate(features):\n",
    "    # Add a new subplot iteratively\n",
    "    ax = plt.subplot(4, 3, i+1)\n",
    "    train_df[train_df['target'] == 0][feature].hist(alpha=0.5, label='Non-disaster', bins=40, color='red', density=True, ax=ax)\n",
    "    train_df[train_df['target'] == 1][feature].hist(alpha=0.5, label='Disaster', bins=40, color='blue', density=True, ax=ax)\n",
    "\n",
    "    # Set x_label, y_label, and legend\n",
    "    ax.set_xlabel(feature, fontsize=14)\n",
    "    ax.set_ylabel('Probability Density', fontsize=14)\n",
    "    ax.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def donutplot(value, label, color, title):\n",
    "    plt.pie(value, labels=label, autopct='%1.1f%%', startangle=90, colors=color)\n",
    "    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    plt.axis('equal')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def classwise_comparison_barplot(df, n, feature, non_disaster, disaster, xlabel, ylabel, title):\n",
    "    df_sorted = df.sort_values(by=[non_disaster, disaster], ascending=False)\n",
    "    df_selected = df_sorted.head(n)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(15,50))\n",
    "    sns.barplot(x=non_disaster, y=feature, data=df_selected, label=\"Non-disaster\", color=\"green\")\n",
    "    sns.barplot(x=disaster, y=feature, data=df_selected, label=\"Disaster\", color=\"red\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def word_counter(text_col, search_word):\n",
    "    counter = 0\n",
    "    for text in text_col:\n",
    "        if search_word in str(text).lower().split():\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# Assuming you have 'train_df' loaded with the required columns\n",
    "keyword_df = train_df.groupby(['keyword', 'target'])['text_cleaned'].count().reset_index()\n",
    "keyword_df.rename(columns={'text_cleaned': 'count'}, inplace=True)\n",
    "\n",
    "# Pivot the DataFrame and merge 'target' back for plotting\n",
    "keyword_df = keyword_df.pivot(index='keyword', columns='target', values='count').reset_index()\n",
    "keyword_df.fillna(0, inplace=True)\n",
    "keyword_df.columns = ['keyword', 'count (non-disaster tweets)', 'count (disaster tweets)']\n",
    "keyword_df['count (all tweets)'] = keyword_df['count (non-disaster tweets)'] + keyword_df['count (disaster tweets)']\n",
    "\n",
    "classwise_comparison_barplot(df=keyword_df,\n",
    "                             n=200,  # Adjust for the desired number of top keywords\n",
    "                             feature='keyword',\n",
    "                             non_disaster='count (non-disaster tweets)',\n",
    "                             disaster='count (disaster tweets)',\n",
    "                             xlabel='count of tweets',\n",
    "                             ylabel='keyword',\n",
    "                             title='Top 200 keyword-count (in decreasing order of total count)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Null Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns='location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  first split the train and test to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize numerical features (before Word2Vec embeddings)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test data\n",
    "train_data[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "            'avg_word_len', 'propn_count', 'punc_per']] = scaler.fit_transform(\n",
    "            train_data[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "                        'avg_word_len', 'propn_count', 'punc_per']])\n",
    "\n",
    "test_data[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "           'avg_word_len', 'propn_count', 'punc_per']] = scaler.transform(\n",
    "           test_data[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "                      'avg_word_len', 'propn_count', 'punc_per']])\n",
    "\n",
    "# Step 3: Tokenize the text_cleaned column (for Word2Vec)\n",
    "train_data['tokenized_text'] = train_data['text_cleaned'].apply(lambda x: x.split())\n",
    "test_data['tokenized_text'] = test_data['text_cleaned'].apply(lambda x: x.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- applying word2ve on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model on the training data only\n",
    "w2v_model = Word2Vec(sentences=train_data['tokenized_text'], vector_size=100, window=8, min_count=1, workers=4)\n",
    "\n",
    "# Generate embeddings by averaging Word2Vec vectors for each row\n",
    "def get_w2v_vector(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "train_data['w2v_embedding'] = train_data['tokenized_text'].apply(get_w2v_vector)\n",
    "test_data['w2v_embedding'] = test_data['tokenized_text'].apply(get_w2v_vector)\n",
    "\n",
    "# Step 4: Convert keywords into numerical form\n",
    "unique_keywords = train_data['keyword'].unique()\n",
    "keyword_mapping = {keyword: idx for idx, keyword in enumerate(unique_keywords)}\n",
    "train_data['keyword_num'] = train_data['keyword'].map(keyword_mapping)\n",
    "test_data['keyword_num'] = test_data['keyword'].map(keyword_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Step 5: Combine features for modeling\n",
    "X_train_w2v = np.vstack(train_data['w2v_embedding'].values)\n",
    "X_test_w2v = np.vstack(test_data['w2v_embedding'].values)\n",
    "\n",
    "# Extract other features (already normalized in Step 2)\n",
    "X_train_other = train_data[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "                            'avg_word_len', 'propn_count', 'punc_per', 'keyword_num']].values\n",
    "X_test_other = test_data[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "                          'avg_word_len', 'propn_count', 'punc_per', 'keyword_num']].values\n",
    "\n",
    "# # Combine the Word2Vec embeddings with the normalized other features\n",
    "X_train = np.hstack([X_train_w2v, X_train_other])\n",
    "X_test = np.hstack([X_test_w2v, X_test_other])\n",
    "\n",
    "y_train = train_data['target'].values\n",
    "y_test = test_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Ensure you have your train-test split data ready\n",
    "# X_train, X_test, y_train, y_test should be pre-defined\n",
    "random_state = 42\n",
    "\n",
    "# Define classifiers\n",
    "clfs = {\n",
    "    'MultinomialNB': GaussianNB(),\n",
    "    'SVC': SVC(random_state=random_state),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=10000, random_state=random_state),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=random_state),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(n_jobs=-1),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=random_state)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Iterate through classifiers\n",
    "for name, clf in clfs.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Classification Report': class_report,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Optional: Convert results to a DataFrame for better visualization\n",
    "summary_results = pd.DataFrame({\n",
    "    model: {metric: results[model][metric] for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score']}\n",
    "    for model in results\n",
    "}).T\n",
    "\n",
    "print(\"\\nSummary Results:\\n\", summary_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sele1ction:\n",
    "    - here best accuracy gives KNeighborsClassifier, RandomForestClassifier and GradientBoostingClassifier\n",
    "    - on this three RandomForest gives best accuracy compare to those\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter tunning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV with 10 iterations and 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Train a model using best params\n",
    "# model =RandomForestClassifier('n_estimators'=150, 'min_samples_split'=5, 'min_samples_leaf'= 1, 'max_features'= 'sqrt', 'max_depth'= 15)\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=150, \n",
    "    min_samples_split=5, \n",
    "    min_samples_leaf=1, \n",
    "    max_features='sqrt', \n",
    "    max_depth=15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now Checking Models Performance Using Unseen Data which is in test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=test_df.drop(columns='location')\n",
    "test_df['keyword'].fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize numerical features in test_df (using the scaler fitted on train data)\n",
    "test_df[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "         'avg_word_len', 'propn_count', 'punc_per']] = scaler.transform(\n",
    "         test_df[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "                  'avg_word_len', 'propn_count', 'punc_per']])\n",
    "\n",
    "# Step 2: Tokenize the 'text_cleaned' column (for Word2Vec)\n",
    "test_df['tokenized_text'] = test_df['text_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Step 3: Generate Word2Vec embeddings for the unseen test data\n",
    "test_df['w2v_embedding'] = test_df['tokenized_text'].apply(get_w2v_vector)\n",
    "\n",
    "# Step 4: Convert keywords into numerical form (same as done in training phase)\n",
    "test_df['keyword_num'] = test_df['keyword'].map(keyword_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Step 5: Extract other features for the test data (already normalized)\n",
    "X_test_w2v_unseen = np.vstack(test_df['w2v_embedding'].values)\n",
    "X_test_other_unseen = test_df[['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count',\n",
    "                                'avg_word_len', 'propn_count', 'punc_per', 'keyword_num']].values\n",
    "\n",
    "# Combine the Word2Vec embeddings with the normalized other features\n",
    "X_test_unseen = np.hstack([X_test_w2v_unseen, X_test_other_unseen])\n",
    "\n",
    "# Step 6: Make predictions using the trained model\n",
    "y_pred_unseen = model.predict(X_test_unseen)\n",
    "\n",
    "# Step 7: Output predictions for the unseen test data\n",
    "test_df['predicted_target'] = y_pred_unseen\n",
    "\n",
    "# Display the predictions\n",
    "print(test_df[[ 'predicted_target']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take only id and predicted_target \n",
    "result=test_df[['id','predicted_target']]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging sample_df and result DataFrame on the base of id \n",
    "final=pd.merge(sample_df,result,on='id')\n",
    "final=final.drop(columns='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column name as target\n",
    "submission=final.rename(columns={'predicted_target':'target'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Download the file to your local machine\n",
    "files.download('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summery:\n",
    "    \n",
    "\n",
    "    - on this dataset RandomForest gives best accuracy which is 0.7246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
