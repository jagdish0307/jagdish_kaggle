{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')\n",
    "sample_df=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_df\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "train_df['keyword'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['location'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test_df\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['keyword'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['location'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test_df\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing data\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicated data\n",
    "test_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.pie(train_df['target'].value_counts(), labels=['Non-disaster', 'Disaster'], autopct='%0.2f')\n",
    "plt.legend()  # Adds a legend\n",
    "plt.title('Distribution of Disaster Tweets')  # Adds a descriptive title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random state\n",
    "random_state = 4041\n",
    "\n",
    "# import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# concat all the text for each labels\n",
    "non_disaster_text = [''.join(t) for t in train_df[train_df['target']==0]['text']]\n",
    "non_disaster_strings = ' '.join(map(str, non_disaster_text))\n",
    "disaster_text = [''.join(t) for t in train_df[train_df['target']==1]['text']]\n",
    "disaster_strings = ' '.join(map(str, disaster_text))\n",
    "\n",
    "# generate word clouds\n",
    "non_disaster_cloud = WordCloud(width=800, height=400, max_words=500, background_color='white', random_state=random_state).generate(non_disaster_strings)\n",
    "disaster_cloud = WordCloud(width=800, height=400, max_words=500, random_state=random_state).generate(disaster_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots for the generated clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize = (20,20))\n",
    "axes[0].imshow(non_disaster_cloud, interpolation='bilinear')\n",
    "axes[1].imshow(disaster_cloud, interpolation='bilinear')\n",
    "\n",
    "# turn the axis off\n",
    "[ax.axis('off') for ax in axes]\n",
    "\n",
    "# add titles\n",
    "axes[0].set_title('Non-disaster Tweets', fontsize=16)\n",
    "axes[1].set_title('Disaster Tweets', fontsize=16)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-remove urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    text = re.sub(r'((?:https?|ftp|file)://[-\\w\\d+=&@#/%?~|!:;\\.,]*)', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text'].apply(remove_url)\n",
    "test_df['text_cleaned'] = test_df['text'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_HTML)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rmove Characters References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(text):\n",
    "    text = re.sub(r'&[a-zA-Z]+;?', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_references)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Non-printable Characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.printable\n",
    "\n",
    "def remove_non_printable(text):\n",
    "    text = ''.join([word for word in text if word in string.printable])\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_non_printable)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_non_printable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Numeric Values\n",
    "Remove numeric values, including mixtures of alphabetical characters and numeric values such as 'M194', '5km'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_num)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_num)\n",
    "\n",
    "train_df.tail()\n",
    "\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Below are 10 features we're going to create:\n",
    "\n",
    "- Number of sentences\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Number of hashtags\n",
    "- Number of mentions\n",
    "- Number of all caps words\n",
    "- Average length of words\n",
    "- Number of proper nouns (PROPN)\n",
    "- Number of non-proper nouns (NOUN)\n",
    "- Percentage of characters that are punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Number of SentencesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# create a new feature for the number of sentences in each Tweet\n",
    "train_df['sent_count'] = train_df['text'].apply(nltk.tokenize.sent_tokenize).apply(len)\n",
    "test_df['sent_count'] = test_df['text'].apply(nltk.tokenize.sent_tokenize).apply(len)\n",
    "\n",
    "# create a new feature for the number of words\n",
    "train_df['word_count'] = train_df['text'].apply(nltk.tokenize.word_tokenize).apply(len)\n",
    "test_df['word_count'] = test_df['text'].apply(nltk.tokenize.word_tokenize).apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature for the number of characters excluding white spaces\n",
    "train_df['char_count'] = train_df['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "test_df['char_count'] = test_df['text'].apply(lambda x: len(x) - x.count(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns the number of hashtags in a string\n",
    "def hash_count(string):\n",
    "    words = string.split()\n",
    "    hashtags = [w for w in words if w.startswith('#')]\n",
    "    return len(hashtags)\n",
    "\n",
    "# create a new feature for the number of hashtags\n",
    "train_df['hash_count'] = train_df['text'].apply(hash_count)\n",
    "test_df['hash_count'] = test_df['text'].apply(hash_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the number of mentions in a string\n",
    "def ment_count(string):\n",
    "    words = string.split()\n",
    "    mentions = [w for w in words if w.startswith('@')]\n",
    "    return len(mentions)\n",
    "\n",
    "# create a new feature for the number of mentions\n",
    "train_df['ment_count'] = train_df['text'].apply(ment_count)\n",
    "test_df['ment_count'] = test_df['text'].apply(ment_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of All Caps Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def all_caps_count(string):\n",
    "    words = string.split()\n",
    "    pattern = re.compile(r'\\b[A-Z]{2,}\\b')  # Matches words with 2 or more consecutive uppercase letters\n",
    "    caps_words = [word for word in words if pattern.fullmatch(word)]\n",
    "    return len(caps_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Length of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns the average length of words\n",
    "def avg_word_len(string):\n",
    "    words = string.split()\n",
    "    total_len = sum([len(words[i]) for i in range(len(words))])\n",
    "    avg_len = round(total_len / len(words), 2)\n",
    "    return avg_len\n",
    "\n",
    "# create a new feature for the average length of words\n",
    "train_df['avg_word_len'] = train_df['text'].apply(avg_word_len)\n",
    "test_df['avg_word_len'] = test_df['text'].apply(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# define a function using nltk that returns the number of proper nouns in the text\n",
    "def propn_count_nltk(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = [token for token in nltk.pos_tag(tokens)]\n",
    "    propn_count = len([token for (token, tag) in tagged if tag == 'NNP' or tag == 'NNPS'])\n",
    "    return propn_count\n",
    "\n",
    "# create a new feature for the number of proper nouns\n",
    "train_df['propn_count_nltk'] = train_df['text'].apply(propn_count_nltk)\n",
    "test_df['propn_count_nltk'] = test_df['text'].apply(propn_count_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned', 'propn_count_nltk']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test how nltk worked with the first text\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token, tag) for (token, tag) in nltk.pos_tag(nltk.word_tokenize(string)) if tag == 'NNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test how nltk works with the first text after lowercasing it\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token, tag) for (token, tag) in nltk.pos_tag(nltk.word_tokenize(string.lower())) if tag == 'NNP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# check the same string with spaCy\n",
    "string = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "print([(token.text, token.pos_) for token in nlp(string) if token.pos_=='PROPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns number of proper nouns with spaCy\n",
    "def propn_count(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "# create a new feature for numbers of proper nouns\n",
    "train_df['propn_count'] = train_df['text'].apply(propn_count)\n",
    "test_df['propn_count'] = test_df['text'].apply(propn_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove 'propn_count_nltk' columns\n",
    "train_df = train_df.drop(['propn_count_nltk'], axis=1)\n",
    "test_df = test_df.drop(['propn_count_nltk'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned', 'propn_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Non-proper Nouns (NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that returns number of non-proper nouns\n",
    "def noun_count(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "# create a new feature for numbers of non-proper nouns\n",
    "train_df['noun_count'] = train_df['text'].apply(noun_count)\n",
    "test_df['noun_count'] = test_df['text'].apply(noun_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of Characters that are Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# define a function that returns the percentage of punctuation\n",
    "def punc_per(text):\n",
    "    total_count = len(text) - text.count(\" \")\n",
    "    punc_count = sum([1 for c in text if c in string.punctuation])\n",
    "    if punc_count != 0 and total_count != 0:\n",
    "        return round(punc_count / total_count * 100, 2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# create a new feature for the percentage of punctuation in text\n",
    "train_df['punc_per'] = train_df['text'].apply(punc_per)\n",
    "test_df['punc_per'] = test_df['text'].apply(punc_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "test_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Part II\n",
    "\n",
    "Let's resume our text preprocessing and lemmatize the text and make it lowercase.\n",
    "\n",
    "We'll also remove repeated characters in elongated words, as well as mentions, stopwords, and punctuation.\n",
    "We'll keep hashtags as they may provide valuable insights in this particular project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x:' '.join([t.lemma_ for t in nlp(x)]))\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(lambda x:' '.join([t.lemma_ for t in nlp(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase the text\n",
    "train_df['text_cleaned'] = [t.lower() for t in train_df['text_cleaned']]\n",
    "test_df['text_cleaned'] = [t.lower() for t in test_df['text_cleaned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Repeated Charcters in Elongated Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that removes repeated characters in elongated words\n",
    "def remove_repeated(text):\n",
    "    elongated = re.compile(r'(\\S*?)([a-z])\\2{2,}')\n",
    "    text = elongated.sub(r'\\1\\2', text)\n",
    "    return text\n",
    "\n",
    "# remove repeated characters in elongated words\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_repeated)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_repeated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Mentions\n",
    "List item\n",
    "\n",
    "List item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that removes mentions\n",
    "def remove_mention(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove mentions\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_mention)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that removes stopwords\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    text_nostop = ' '.join([token for token in text.split() if token not in stopwords])\n",
    "    return text_nostop\n",
    "\n",
    "# remove stopwords\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_stopwords)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation\n",
    " List item\n",
    " \n",
    " List item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    punct = string.punctuation\n",
    "    text_nospunct = ' '.join([token for token in text.split() if token not in punct])\n",
    "    return text_nospunct\n",
    "\n",
    "# remove punctuation\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(remove_punct)\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "train_df[['id', 'text', 'text_cleaned']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw vs. Preprocessed Text with Word Clouds\n",
    "Let's generate word clouds of preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the preprocessed text for both labels\n",
    "non_disaster_processed = [''.join(t) for t in train_df[train_df['target']==0]['text_cleaned']]\n",
    "non_disaster_processed_s = ' '.join(map(str, non_disaster_processed))\n",
    "disaster_processed = [''.join(t) for t in train_df[train_df['target']==1]['text_cleaned']]\n",
    "disaster_processed_s = ' '.join(map(str, disaster_processed))\n",
    "\n",
    "# generate word clouds of the preprocessed text\n",
    "non_disaster_processed_wc = WordCloud(width=800, height=400, max_words=500, background_color='white', random_state=random_state).generate(non_disaster_processed_s)\n",
    "disaster_processed_wc = WordCloud(width=800, height=400, max_words=500, random_state=random_state).generate(disaster_processed_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots for the generated clouds\n",
    "fig, axes = plt.subplots(2, 2, figsize = (20,10))\n",
    "axes[0,0].imshow(non_disaster_cloud, interpolation='bilinear')\n",
    "axes[0,1].imshow(disaster_cloud, interpolation='bilinear')\n",
    "axes[1,0].imshow(non_disaster_processed_wc, interpolation='bilinear')\n",
    "axes[1,1].imshow(disaster_processed_wc, interpolation='bilinear')\n",
    "\n",
    "# turn the axis off\n",
    "[ax.axis('off') for ax in axes.ravel()]\n",
    "\n",
    "# add titles\n",
    "axes[0,0].set_title('Non-disaster Tweets (raw)', fontsize=16)\n",
    "axes[0,1].set_title('Disaster Tweets (raw)', fontsize=16)\n",
    "axes[1,0].set_title('Non-disaster Tweets (preprocessed)', fontsize=16)\n",
    "axes[1,1].set_title('Disaster Tweets (preprocessed)', fontsize=16)\n",
    "\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's easier to see the frequently used words that actually are meaningful. It also seems like more disaster-related words are showing on the word cloud of real disaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Visualizing Differences**\n",
    "Let's visualize some of the features we've created and see if there are easy-to-tell differences between disaster and non-disaster Tweets in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features exist in the DataFrame\n",
    "features = ['sent_count', 'word_count', 'char_count', 'hash_count', 'ment_count', 'all_caps_count',\n",
    "            'avg_word_len', 'propn_count', 'noun_count', 'punc_per']\n",
    "\n",
    "# Remove features not in the DataFrame\n",
    "features = [feature for feature in features if feature in train_df.columns]\n",
    "\n",
    "# Create the figure\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Adjust the height of the padding between subplots to avoid overlapping\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Add a centered suptitle to the figure\n",
    "plt.suptitle(\"Difference in Features, Disaster vs. Non-disaster\", fontsize=20, y=0.91)\n",
    "\n",
    "# Generate the histograms in a for loop\n",
    "for i, feature in enumerate(features):\n",
    "    # Add a new subplot iteratively\n",
    "    ax = plt.subplot(4, 3, i+1)\n",
    "    train_df[train_df['target'] == 0][feature].hist(alpha=0.5, label='Non-disaster', bins=40, color='red', density=True, ax=ax)\n",
    "    train_df[train_df['target'] == 1][feature].hist(alpha=0.5, label='Disaster', bins=40, color='blue', density=True, ax=ax)\n",
    "\n",
    "    # Set x_label, y_label, and legend\n",
    "    ax.set_xlabel(feature, fontsize=14)\n",
    "    ax.set_ylabel('Probability Density', fontsize=14)\n",
    "    ax.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def donutplot(value, label, color, title):\n",
    "    plt.pie(value, labels=label, autopct='%1.1f%%', startangle=90, colors=color)\n",
    "    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    plt.axis('equal')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def classwise_comparison_barplot(df, n, feature, non_disaster, disaster, xlabel, ylabel, title):\n",
    "    df_sorted = df.sort_values(by=[non_disaster, disaster], ascending=False)\n",
    "    df_selected = df_sorted.head(n)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(15,50))\n",
    "    sns.barplot(x=non_disaster, y=feature, data=df_selected, label=\"Non-disaster\", color=\"green\")\n",
    "    sns.barplot(x=disaster, y=feature, data=df_selected, label=\"Disaster\", color=\"red\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def word_counter(text_col, search_word):\n",
    "    counter = 0\n",
    "    for text in text_col:\n",
    "        if search_word in str(text).lower().split():\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# Assuming you have 'train_df' loaded with the required columns\n",
    "keyword_df = train_df.groupby(['keyword', 'target'])['text_cleaned'].count().reset_index()\n",
    "keyword_df.rename(columns={'text_cleaned': 'count'}, inplace=True)\n",
    "\n",
    "# Pivot the DataFrame and merge 'target' back for plotting\n",
    "keyword_df = keyword_df.pivot(index='keyword', columns='target', values='count').reset_index()\n",
    "keyword_df.fillna(0, inplace=True)\n",
    "keyword_df.columns = ['keyword', 'count (non-disaster tweets)', 'count (disaster tweets)']\n",
    "keyword_df['count (all tweets)'] = keyword_df['count (non-disaster tweets)'] + keyword_df['count (disaster tweets)']\n",
    "\n",
    "classwise_comparison_barplot(df=keyword_df,\n",
    "                             n=200,  # Adjust for the desired number of top keywords\n",
    "                             feature='keyword',\n",
    "                             non_disaster='count (non-disaster tweets)',\n",
    "                             disaster='count (disaster tweets)',\n",
    "                             xlabel='count of tweets',\n",
    "                             ylabel='keyword',\n",
    "                             title='Top 200 keyword-count (in decreasing order of total count)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Null Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns='location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
